{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7073228,"sourceType":"datasetVersion","datasetId":4073685}],"dockerImageVersionId":30262,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-28T15:08:37.179670Z","iopub.execute_input":"2023-11-28T15:08:37.180322Z","iopub.status.idle":"2023-11-28T15:08:38.191381Z","shell.execute_reply.started":"2023-11-28T15:08:37.180231Z","shell.execute_reply":"2023-11-28T15:08:38.189977Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/elonmusk-2019-tweets/2019.csv\nTue Nov 28 15:08:38 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   31C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget -O prompts.csv https://raw.githubusercontent.com/krea-ai/open-prompts/main/data/1k.csv","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:53:56.048158Z","iopub.execute_input":"2022-10-03T14:53:56.050989Z","iopub.status.idle":"2022-10-03T14:53:57.441739Z","shell.execute_reply.started":"2022-10-03T14:53:56.050953Z","shell.execute_reply":"2022-10-03T14:53:57.439981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-10-03T14:54:01.828516Z","iopub.execute_input":"2022-10-03T14:54:01.829027Z","iopub.status.idle":"2022-10-03T14:54:03.055341Z","shell.execute_reply.started":"2022-10-03T14:54:01.828981Z","shell.execute_reply":"2022-10-03T14:54:03.053672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming the dataset is in the '/kaggle/input/' directory\nfile_path = '/kaggle/input/elonmusk-2019-tweets/2019.csv'","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:09:30.118852Z","iopub.execute_input":"2023-11-28T15:09:30.119905Z","iopub.status.idle":"2023-11-28T15:09:30.125392Z","shell.execute_reply.started":"2023-11-28T15:09:30.119857Z","shell.execute_reply":"2023-11-28T15:09:30.124140Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"tweets = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:10:05.188376Z","iopub.execute_input":"2023-11-28T15:10:05.189415Z","iopub.status.idle":"2023-11-28T15:10:05.355795Z","shell.execute_reply.started":"2023-11-28T15:10:05.189365Z","shell.execute_reply":"2023-11-28T15:10:05.354763Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tweets.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:10:10.403328Z","iopub.execute_input":"2023-11-28T15:10:10.403719Z","iopub.status.idle":"2023-11-28T15:10:10.444876Z","shell.execute_reply.started":"2023-11-28T15:10:10.403687Z","shell.execute_reply":"2023-11-28T15:10:10.443705Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                   id      conversation_id    created_at  \\\n0           0  1211071324518531072  1210918835861241856  1.577577e+12   \n1           1  1211069054779301894  1210918835861241856  1.577576e+12   \n2           2  1211064937004589056  1210918835861241856  1.577575e+12   \n3           3  1211054942192119808  1210918835861241856  1.577573e+12   \n4           4  1211051740562366464  1210774027054145536  1.577572e+12   \n\n                  date  timezone  place  \\\n0  2019-12-28 23:47:59         0    NaN   \n1  2019-12-28 23:38:57         0    NaN   \n2  2019-12-28 23:22:36         0    NaN   \n3  2019-12-28 22:42:53         0    NaN   \n4  2019-12-28 22:30:09         0    NaN   \n\n                                               tweet language hashtags  ...  \\\n0  @Joe__Wakefield @austinbarnard45 @tjq1190 @tyg...       en       []  ...   \n1  @austinbarnard45 @tjq1190 @tyger_cyber @fawful...       en       []  ...   \n2  @IrinaAntony @tjq1190 @tyger_cyber @fawfulfan ...       en       []  ...   \n3  @tjq1190 @tyger_cyber @fawfulfan @_Mikemo He d...       en       []  ...   \n4                           @geofficient Pretty much       en       []  ...   \n\n  geo  source  user_rt_id user_rt retweet_id  \\\n0 NaN     NaN         NaN     NaN        NaN   \n1 NaN     NaN         NaN     NaN        NaN   \n2 NaN     NaN         NaN     NaN        NaN   \n3 NaN     NaN         NaN     NaN        NaN   \n4 NaN     NaN         NaN     NaN        NaN   \n\n                                            reply_to  retweet_date translate  \\\n0  [{'screen_name': 'austinbarnard45', 'name': 'A...           NaN       NaN   \n1  [{'screen_name': 'austinbarnard45', 'name': 'A...           NaN       NaN   \n2  [{'screen_name': 'IrinaAntony', 'name': '✨⭐️✨'...           NaN       NaN   \n3  [{'screen_name': 'tjq1190', 'name': 'Tyler J. ...           NaN       NaN   \n4  [{'screen_name': 'geofficient', 'name': 'Geoff...           NaN       NaN   \n\n  trans_src trans_dest  \n0       NaN        NaN  \n1       NaN        NaN  \n2       NaN        NaN  \n3       NaN        NaN  \n4       NaN        NaN  \n\n[5 rows x 39 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>conversation_id</th>\n      <th>created_at</th>\n      <th>date</th>\n      <th>timezone</th>\n      <th>place</th>\n      <th>tweet</th>\n      <th>language</th>\n      <th>hashtags</th>\n      <th>...</th>\n      <th>geo</th>\n      <th>source</th>\n      <th>user_rt_id</th>\n      <th>user_rt</th>\n      <th>retweet_id</th>\n      <th>reply_to</th>\n      <th>retweet_date</th>\n      <th>translate</th>\n      <th>trans_src</th>\n      <th>trans_dest</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1211071324518531072</td>\n      <td>1210918835861241856</td>\n      <td>1.577577e+12</td>\n      <td>2019-12-28 23:47:59</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>@Joe__Wakefield @austinbarnard45 @tjq1190 @tyg...</td>\n      <td>en</td>\n      <td>[]</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[{'screen_name': 'austinbarnard45', 'name': 'A...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1211069054779301894</td>\n      <td>1210918835861241856</td>\n      <td>1.577576e+12</td>\n      <td>2019-12-28 23:38:57</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>@austinbarnard45 @tjq1190 @tyger_cyber @fawful...</td>\n      <td>en</td>\n      <td>[]</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[{'screen_name': 'austinbarnard45', 'name': 'A...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1211064937004589056</td>\n      <td>1210918835861241856</td>\n      <td>1.577575e+12</td>\n      <td>2019-12-28 23:22:36</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>@IrinaAntony @tjq1190 @tyger_cyber @fawfulfan ...</td>\n      <td>en</td>\n      <td>[]</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[{'screen_name': 'IrinaAntony', 'name': '✨⭐️✨'...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1211054942192119808</td>\n      <td>1210918835861241856</td>\n      <td>1.577573e+12</td>\n      <td>2019-12-28 22:42:53</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>@tjq1190 @tyger_cyber @fawfulfan @_Mikemo He d...</td>\n      <td>en</td>\n      <td>[]</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[{'screen_name': 'tjq1190', 'name': 'Tyler J. ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1211051740562366464</td>\n      <td>1210774027054145536</td>\n      <td>1.577572e+12</td>\n      <td>2019-12-28 22:30:09</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>@geofficient Pretty much</td>\n      <td>en</td>\n      <td>[]</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[{'screen_name': 'geofficient', 'name': 'Geoff...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 39 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tweets.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:11:27.334887Z","iopub.execute_input":"2023-11-28T15:11:27.335276Z","iopub.status.idle":"2023-11-28T15:11:27.342301Z","shell.execute_reply.started":"2023-11-28T15:11:27.335244Z","shell.execute_reply":"2023-11-28T15:11:27.341271Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Index(['Unnamed: 0', 'id', 'conversation_id', 'created_at', 'date', 'timezone',\n       'place', 'tweet', 'language', 'hashtags', 'cashtags', 'user_id',\n       'user_id_str', 'username', 'name', 'day', 'hour', 'link', 'urls',\n       'photos', 'video', 'thumbnail', 'retweet', 'nlikes', 'nreplies',\n       'nretweets', 'quote_url', 'search', 'near', 'geo', 'source',\n       'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date',\n       'translate', 'trans_src', 'trans_dest'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"Majority of features are useless for use, thus we will just remove them, and will keep just 'tweet' feature.","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:13:10.697703Z","iopub.execute_input":"2023-11-28T15:13:10.698608Z","iopub.status.idle":"2023-11-28T15:13:10.704940Z","shell.execute_reply.started":"2023-11-28T15:13:10.698571Z","shell.execute_reply":"2023-11-28T15:13:10.703407Z"}}},{"cell_type":"code","source":"tweets = tweets[['id', 'tweet']]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:12:26.407834Z","iopub.execute_input":"2023-11-28T15:12:26.408276Z","iopub.status.idle":"2023-11-28T15:12:26.417336Z","shell.execute_reply.started":"2023-11-28T15:12:26.408242Z","shell.execute_reply":"2023-11-28T15:12:26.416270Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"# Use regular expression to remove all the tagged user accounts as they donot contribute to the text\nimport re\n\ndef clean_tweet(tweet):\n    # Remove mentions\n    mention_pattern = r'@\\w+'\n    cleaned_tweet = re.sub(mention_pattern, '', tweet)\n\n    # Remove extra spaces\n    cleaned_tweet = ' '.join(cleaned_tweet.split())\n\n    # Remove hyperlinks\n    hyperlink_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    cleaned_tweet = re.sub(hyperlink_pattern, '', cleaned_tweet)\n\n    return cleaned_tweet","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:40:36.393840Z","iopub.execute_input":"2023-11-28T15:40:36.394626Z","iopub.status.idle":"2023-11-28T15:40:36.401357Z","shell.execute_reply.started":"2023-11-28T15:40:36.394587Z","shell.execute_reply":"2023-11-28T15:40:36.400369Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"tweets.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:40:39.268816Z","iopub.execute_input":"2023-11-28T15:40:39.269902Z","iopub.status.idle":"2023-11-28T15:40:39.284956Z","shell.execute_reply.started":"2023-11-28T15:40:39.269839Z","shell.execute_reply":"2023-11-28T15:40:39.283865Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"id       0\ntweet    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"tweets.reset_index(inplace = True,drop = True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:21:02.809995Z","iopub.execute_input":"2023-11-28T15:21:02.810477Z","iopub.status.idle":"2023-11-28T15:21:02.815777Z","shell.execute_reply.started":"2023-11-28T15:21:02.810442Z","shell.execute_reply":"2023-11-28T15:21:02.814647Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tweets['tweet'] = tweets['tweet'].apply(clean_tweet)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:42:04.741188Z","iopub.execute_input":"2023-11-28T15:42:04.741613Z","iopub.status.idle":"2023-11-28T15:42:04.780448Z","shell.execute_reply.started":"2023-11-28T15:42:04.741576Z","shell.execute_reply":"2023-11-28T15:42:04.779411Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"tweets.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:42:06.240396Z","iopub.execute_input":"2023-11-28T15:42:06.240794Z","iopub.status.idle":"2023-11-28T15:42:06.251442Z","shell.execute_reply.started":"2023-11-28T15:42:06.240762Z","shell.execute_reply":"2023-11-28T15:42:06.250299Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"                    id                                              tweet\n0  1211071324518531072  This is a pretty awful lie. I left South Afric...\n1  1211069054779301894  This person blocked me, so can’t read the twee...\n2  1211064937004589056  We started Zip2 with ~$2k from me plus my over...\n3  1211054942192119808  He didn’t own an emerald mine &amp; I worked m...\n7  1211029491188948992  Hopefully working in Caribbean by end of 2020....","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1211071324518531072</td>\n      <td>This is a pretty awful lie. I left South Afric...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1211069054779301894</td>\n      <td>This person blocked me, so can’t read the twee...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1211064937004589056</td>\n      <td>We started Zip2 with ~$2k from me plus my over...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1211054942192119808</td>\n      <td>He didn’t own an emerald mine &amp;amp; I worked m...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1211029491188948992</td>\n      <td>Hopefully working in Caribbean by end of 2020....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tweets = tweets.loc[tweets.tweet.str.split(' ').str.len() > 10]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:40:56.843020Z","iopub.execute_input":"2023-11-28T15:40:56.843460Z","iopub.status.idle":"2023-11-28T15:40:56.869323Z","shell.execute_reply.started":"2023-11-28T15:40:56.843425Z","shell.execute_reply":"2023-11-28T15:40:56.868021Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"tweets.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:40:58.631663Z","iopub.execute_input":"2023-11-28T15:40:58.632475Z","iopub.status.idle":"2023-11-28T15:40:58.640058Z","shell.execute_reply.started":"2023-11-28T15:40:58.632430Z","shell.execute_reply":"2023-11-28T15:40:58.638832Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"(4568, 2)"},"metadata":{}}]},{"cell_type":"code","source":"! pip install aitextgen -q","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:25:43.100278Z","iopub.execute_input":"2023-11-28T15:25:43.101277Z","iopub.status.idle":"2023-11-28T15:26:00.022444Z","shell.execute_reply.started":"2023-11-28T15:25:43.101233Z","shell.execute_reply":"2023-11-28T15:26:00.021327Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"model=\"EleutherAI/gpt-neo-125M\"","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:26:14.538803Z","iopub.execute_input":"2023-11-28T15:26:14.539690Z","iopub.status.idle":"2023-11-28T15:26:14.544388Z","shell.execute_reply.started":"2023-11-28T15:26:14.539646Z","shell.execute_reply":"2023-11-28T15:26:14.543374Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"prompts_gt6 = prompts_gt6.drop('raw_data', axis = 1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T15:07:19.973209Z","iopub.execute_input":"2022-10-03T15:07:19.973622Z","iopub.status.idle":"2022-10-03T15:07:19.981938Z","shell.execute_reply.started":"2022-10-03T15:07:19.973577Z","shell.execute_reply":"2022-10-03T15:07:19.980071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.to_csv(\"input_text_cleaned.txt\", columns=[\"tweet\"], header=False, index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:41:12.014680Z","iopub.execute_input":"2023-11-28T15:41:12.015596Z","iopub.status.idle":"2023-11-28T15:41:12.052338Z","shell.execute_reply.started":"2023-11-28T15:41:12.015557Z","shell.execute_reply":"2023-11-28T15:41:12.051246Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"from aitextgen.TokenDataset import TokenDataset","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:26:29.312410Z","iopub.execute_input":"2023-11-28T15:26:29.312802Z","iopub.status.idle":"2023-11-28T15:26:37.858130Z","shell.execute_reply.started":"2023-11-28T15:26:29.312774Z","shell.execute_reply":"2023-11-28T15:26:37.856978Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"data = TokenDataset('./input_text_cleaned.txt', line_by_line=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:27:02.643339Z","iopub.execute_input":"2023-11-28T15:27:02.643728Z","iopub.status.idle":"2023-11-28T15:27:03.124434Z","shell.execute_reply.started":"2023-11-28T15:27:02.643696Z","shell.execute_reply":"2023-11-28T15:27:03.123363Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4568 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"246a3dc9a3ae40639d7ae9ae69610ddc"}},"metadata":{}}]},{"cell_type":"code","source":"from aitextgen import aitextgen","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:27:11.846539Z","iopub.execute_input":"2023-11-28T15:27:11.846934Z","iopub.status.idle":"2023-11-28T15:27:11.852098Z","shell.execute_reply.started":"2023-11-28T15:27:11.846899Z","shell.execute_reply":"2023-11-28T15:27:11.850954Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"ai = aitextgen(tf_gpt2=\"124M\",  to_gpu=True)\n# ai = aitextgen(model=\"EleutherAI/gpt-neo-125M\", to_gpu=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:30:10.455821Z","iopub.execute_input":"2023-11-28T15:30:10.456953Z","iopub.status.idle":"2023-11-28T15:31:14.723685Z","shell.execute_reply.started":"2023-11-28T15:30:10.456906Z","shell.execute_reply":"2023-11-28T15:31:14.722815Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching checkpoint:   0%|          | 0.00/77.0 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07444a7865884b53ab0bd130941cb66e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching hparams.json:   0%|          | 0.00/90.0 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d8eaab2c52d4cb2aff92a90d1de975a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching model.ckpt.data-00000-of-00001:   0%|          | 0.00/498M [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fe87a8c5e084419977070a197a289c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching model.ckpt.index:   0%|          | 0.00/5.21k [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f7ae126c8848bb8a395b50e1c86e6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching model.ckpt.meta:   0%|          | 0.00/471k [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8c1b98b97c241b480054f904edf15be"}},"metadata":{}},{"name":"stderr","text":"Converting TensorFlow checkpoint from /kaggle/working/aitextgen/124M\nLoading TF weight model/h0/attn/c_attn/b with shape [2304]\nLoading TF weight model/h0/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h0/attn/c_proj/b with shape [768]\nLoading TF weight model/h0/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h0/ln_1/b with shape [768]\nLoading TF weight model/h0/ln_1/g with shape [768]\nLoading TF weight model/h0/ln_2/b with shape [768]\nLoading TF weight model/h0/ln_2/g with shape [768]\nLoading TF weight model/h0/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h0/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h0/mlp/c_proj/b with shape [768]\nLoading TF weight model/h0/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h1/attn/c_attn/b with shape [2304]\nLoading TF weight model/h1/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h1/attn/c_proj/b with shape [768]\nLoading TF weight model/h1/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h1/ln_1/b with shape [768]\nLoading TF weight model/h1/ln_1/g with shape [768]\nLoading TF weight model/h1/ln_2/b with shape [768]\nLoading TF weight model/h1/ln_2/g with shape [768]\nLoading TF weight model/h1/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h1/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h1/mlp/c_proj/b with shape [768]\nLoading TF weight model/h1/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h10/attn/c_attn/b with shape [2304]\nLoading TF weight model/h10/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h10/attn/c_proj/b with shape [768]\nLoading TF weight model/h10/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h10/ln_1/b with shape [768]\nLoading TF weight model/h10/ln_1/g with shape [768]\nLoading TF weight model/h10/ln_2/b with shape [768]\nLoading TF weight model/h10/ln_2/g with shape [768]\nLoading TF weight model/h10/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h10/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h10/mlp/c_proj/b with shape [768]\nLoading TF weight model/h10/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h11/attn/c_attn/b with shape [2304]\nLoading TF weight model/h11/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h11/attn/c_proj/b with shape [768]\nLoading TF weight model/h11/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h11/ln_1/b with shape [768]\nLoading TF weight model/h11/ln_1/g with shape [768]\nLoading TF weight model/h11/ln_2/b with shape [768]\nLoading TF weight model/h11/ln_2/g with shape [768]\nLoading TF weight model/h11/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h11/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h11/mlp/c_proj/b with shape [768]\nLoading TF weight model/h11/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h2/attn/c_attn/b with shape [2304]\nLoading TF weight model/h2/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h2/attn/c_proj/b with shape [768]\nLoading TF weight model/h2/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h2/ln_1/b with shape [768]\nLoading TF weight model/h2/ln_1/g with shape [768]\nLoading TF weight model/h2/ln_2/b with shape [768]\nLoading TF weight model/h2/ln_2/g with shape [768]\nLoading TF weight model/h2/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h2/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h2/mlp/c_proj/b with shape [768]\nLoading TF weight model/h2/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h3/attn/c_attn/b with shape [2304]\nLoading TF weight model/h3/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h3/attn/c_proj/b with shape [768]\nLoading TF weight model/h3/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h3/ln_1/b with shape [768]\nLoading TF weight model/h3/ln_1/g with shape [768]\nLoading TF weight model/h3/ln_2/b with shape [768]\nLoading TF weight model/h3/ln_2/g with shape [768]\nLoading TF weight model/h3/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h3/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h3/mlp/c_proj/b with shape [768]\nLoading TF weight model/h3/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h4/attn/c_attn/b with shape [2304]\nLoading TF weight model/h4/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h4/attn/c_proj/b with shape [768]\nLoading TF weight model/h4/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h4/ln_1/b with shape [768]\nLoading TF weight model/h4/ln_1/g with shape [768]\nLoading TF weight model/h4/ln_2/b with shape [768]\nLoading TF weight model/h4/ln_2/g with shape [768]\nLoading TF weight model/h4/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h4/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h4/mlp/c_proj/b with shape [768]\nLoading TF weight model/h4/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h5/attn/c_attn/b with shape [2304]\nLoading TF weight model/h5/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h5/attn/c_proj/b with shape [768]\nLoading TF weight model/h5/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h5/ln_1/b with shape [768]\nLoading TF weight model/h5/ln_1/g with shape [768]\nLoading TF weight model/h5/ln_2/b with shape [768]\nLoading TF weight model/h5/ln_2/g with shape [768]\nLoading TF weight model/h5/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h5/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h5/mlp/c_proj/b with shape [768]\nLoading TF weight model/h5/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h6/attn/c_attn/b with shape [2304]\nLoading TF weight model/h6/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h6/attn/c_proj/b with shape [768]\nLoading TF weight model/h6/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h6/ln_1/b with shape [768]\nLoading TF weight model/h6/ln_1/g with shape [768]\nLoading TF weight model/h6/ln_2/b with shape [768]\nLoading TF weight model/h6/ln_2/g with shape [768]\nLoading TF weight model/h6/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h6/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h6/mlp/c_proj/b with shape [768]\nLoading TF weight model/h6/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h7/attn/c_attn/b with shape [2304]\nLoading TF weight model/h7/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h7/attn/c_proj/b with shape [768]\nLoading TF weight model/h7/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h7/ln_1/b with shape [768]\nLoading TF weight model/h7/ln_1/g with shape [768]\nLoading TF weight model/h7/ln_2/b with shape [768]\nLoading TF weight model/h7/ln_2/g with shape [768]\nLoading TF weight model/h7/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h7/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h7/mlp/c_proj/b with shape [768]\nLoading TF weight model/h7/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h8/attn/c_attn/b with shape [2304]\nLoading TF weight model/h8/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h8/attn/c_proj/b with shape [768]\nLoading TF weight model/h8/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h8/ln_1/b with shape [768]\nLoading TF weight model/h8/ln_1/g with shape [768]\nLoading TF weight model/h8/ln_2/b with shape [768]\nLoading TF weight model/h8/ln_2/g with shape [768]\nLoading TF weight model/h8/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h8/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h8/mlp/c_proj/b with shape [768]\nLoading TF weight model/h8/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/h9/attn/c_attn/b with shape [2304]\nLoading TF weight model/h9/attn/c_attn/w with shape [1, 768, 2304]\nLoading TF weight model/h9/attn/c_proj/b with shape [768]\nLoading TF weight model/h9/attn/c_proj/w with shape [1, 768, 768]\nLoading TF weight model/h9/ln_1/b with shape [768]\nLoading TF weight model/h9/ln_1/g with shape [768]\nLoading TF weight model/h9/ln_2/b with shape [768]\nLoading TF weight model/h9/ln_2/g with shape [768]\nLoading TF weight model/h9/mlp/c_fc/b with shape [3072]\nLoading TF weight model/h9/mlp/c_fc/w with shape [1, 768, 3072]\nLoading TF weight model/h9/mlp/c_proj/b with shape [768]\nLoading TF weight model/h9/mlp/c_proj/w with shape [1, 3072, 768]\nLoading TF weight model/ln_f/b with shape [768]\nLoading TF weight model/ln_f/g with shape [768]\nLoading TF weight model/wpe with shape [1024, 768]\nLoading TF weight model/wte with shape [50257, 768]\nInitialize PyTorch weight ['h0', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h0', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h0', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h0', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h0', 'ln_1', 'b']\nInitialize PyTorch weight ['h0', 'ln_1', 'g']\nInitialize PyTorch weight ['h0', 'ln_2', 'b']\nInitialize PyTorch weight ['h0', 'ln_2', 'g']\nInitialize PyTorch weight ['h0', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h0', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h0', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h0', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h1', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h1', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h1', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h1', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h1', 'ln_1', 'b']\nInitialize PyTorch weight ['h1', 'ln_1', 'g']\nInitialize PyTorch weight ['h1', 'ln_2', 'b']\nInitialize PyTorch weight ['h1', 'ln_2', 'g']\nInitialize PyTorch weight ['h1', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h1', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h1', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h1', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h10', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h10', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h10', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h10', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h10', 'ln_1', 'b']\nInitialize PyTorch weight ['h10', 'ln_1', 'g']\nInitialize PyTorch weight ['h10', 'ln_2', 'b']\nInitialize PyTorch weight ['h10', 'ln_2', 'g']\nInitialize PyTorch weight ['h10', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h10', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h10', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h10', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h11', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h11', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h11', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h11', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h11', 'ln_1', 'b']\nInitialize PyTorch weight ['h11', 'ln_1', 'g']\nInitialize PyTorch weight ['h11', 'ln_2', 'b']\nInitialize PyTorch weight ['h11', 'ln_2', 'g']\nInitialize PyTorch weight ['h11', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h11', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h11', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h11', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h2', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h2', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h2', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h2', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h2', 'ln_1', 'b']\nInitialize PyTorch weight ['h2', 'ln_1', 'g']\nInitialize PyTorch weight ['h2', 'ln_2', 'b']\nInitialize PyTorch weight ['h2', 'ln_2', 'g']\nInitialize PyTorch weight ['h2', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h2', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h2', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h2', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h3', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h3', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h3', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h3', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h3', 'ln_1', 'b']\nInitialize PyTorch weight ['h3', 'ln_1', 'g']\nInitialize PyTorch weight ['h3', 'ln_2', 'b']\nInitialize PyTorch weight ['h3', 'ln_2', 'g']\nInitialize PyTorch weight ['h3', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h3', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h3', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h3', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h4', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h4', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h4', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h4', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h4', 'ln_1', 'b']\nInitialize PyTorch weight ['h4', 'ln_1', 'g']\nInitialize PyTorch weight ['h4', 'ln_2', 'b']\nInitialize PyTorch weight ['h4', 'ln_2', 'g']\nInitialize PyTorch weight ['h4', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h4', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h4', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h4', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h5', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h5', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h5', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h5', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h5', 'ln_1', 'b']\nInitialize PyTorch weight ['h5', 'ln_1', 'g']\nInitialize PyTorch weight ['h5', 'ln_2', 'b']\nInitialize PyTorch weight ['h5', 'ln_2', 'g']\nInitialize PyTorch weight ['h5', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h5', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h5', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h5', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h6', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h6', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h6', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h6', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h6', 'ln_1', 'b']\nInitialize PyTorch weight ['h6', 'ln_1', 'g']\nInitialize PyTorch weight ['h6', 'ln_2', 'b']\nInitialize PyTorch weight ['h6', 'ln_2', 'g']\nInitialize PyTorch weight ['h6', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h6', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h6', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h6', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h7', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h7', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h7', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h7', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h7', 'ln_1', 'b']\nInitialize PyTorch weight ['h7', 'ln_1', 'g']\nInitialize PyTorch weight ['h7', 'ln_2', 'b']\nInitialize PyTorch weight ['h7', 'ln_2', 'g']\nInitialize PyTorch weight ['h7', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h7', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h7', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h7', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h8', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h8', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h8', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h8', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h8', 'ln_1', 'b']\nInitialize PyTorch weight ['h8', 'ln_1', 'g']\nInitialize PyTorch weight ['h8', 'ln_2', 'b']\nInitialize PyTorch weight ['h8', 'ln_2', 'g']\nInitialize PyTorch weight ['h8', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h8', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h8', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h8', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['h9', 'attn', 'c_attn', 'b']\nInitialize PyTorch weight ['h9', 'attn', 'c_attn', 'w']\nInitialize PyTorch weight ['h9', 'attn', 'c_proj', 'b']\nInitialize PyTorch weight ['h9', 'attn', 'c_proj', 'w']\nInitialize PyTorch weight ['h9', 'ln_1', 'b']\nInitialize PyTorch weight ['h9', 'ln_1', 'g']\nInitialize PyTorch weight ['h9', 'ln_2', 'b']\nInitialize PyTorch weight ['h9', 'ln_2', 'g']\nInitialize PyTorch weight ['h9', 'mlp', 'c_fc', 'b']\nInitialize PyTorch weight ['h9', 'mlp', 'c_fc', 'w']\nInitialize PyTorch weight ['h9', 'mlp', 'c_proj', 'b']\nInitialize PyTorch weight ['h9', 'mlp', 'c_proj', 'w']\nInitialize PyTorch weight ['ln_f', 'b']\nInitialize PyTorch weight ['ln_f', 'g']\nInitialize PyTorch weight ['wpe']\nInitialize PyTorch weight ['wte']\n","output_type":"stream"},{"name":"stdout","text":"Save PyTorch model to aitextgen/pytorch_model.bin\nSave configuration file to aitextgen/config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"# How can we try the performance of model before finetuning","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ai.train('input_text_cleaned.txt',\n         line_by_line=True,\n         from_cache=False,\n         num_steps=500,\n         generate_every=100,\n         save_every=500,\n         save_gdrive=False,\n         learning_rate=1e-3,\n         fp16=False,\n         batch_size=1, \n         )","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:32:58.045173Z","iopub.execute_input":"2023-11-28T15:32:58.046169Z","iopub.status.idle":"2023-11-28T15:34:56.804492Z","shell.execute_reply.started":"2023-11-28T15:32:58.046127Z","shell.execute_reply":"2023-11-28T15:34:56.803251Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4568 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3956f994fbc440a4ac3ed8084afd1006"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:448: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\n  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:259: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n  f\"The `Callback.{hook}` hook was deprecated in v1.6 and\"\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744122f0509c498a881b12e92289c46d"}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m100 steps reached: generating sample texts.\u001b[0m\n==========\n\n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m200 steps reached: generating sample texts.\u001b[0m\n==========\n-push “:                                                                                                                                                                                                                                                          \n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m300 steps reached: generating sample texts.\u001b[0m\n==========\n\n\n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m400 steps reached: generating sample texts.\u001b[0m\n==========\n The impact force was not to your chest, but you were going up supersonic in a good way.\n\n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m500 steps reached: saving model to /trained_model\u001b[0m\n\u001b[1m500 steps reached: generating sample texts.\u001b[0m\n==========\n by itself is also why we cannot be concerned about the future beyond Earth. Those are just guesses for now.\"\n\n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"ai.save()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:35:00.704672Z","iopub.execute_input":"2023-11-28T15:35:00.705685Z","iopub.status.idle":"2023-11-28T15:35:01.478969Z","shell.execute_reply.started":"2023-11-28T15:35:00.705623Z","shell.execute_reply":"2023-11-28T15:35:01.477766Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"prompt_ai = aitextgen(model_folder = '.', to_gpu=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:35:05.204698Z","iopub.execute_input":"2023-11-28T15:35:05.205117Z","iopub.status.idle":"2023-11-28T15:35:07.623540Z","shell.execute_reply.started":"2023-11-28T15:35:05.205084Z","shell.execute_reply":"2023-11-28T15:35:07.622287Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"print(prompt_ai.generate(prompt = \"We have to \"))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:35:39.681739Z","iopub.execute_input":"2023-11-28T15:35:39.682156Z","iopub.status.idle":"2023-11-28T15:35:40.286486Z","shell.execute_reply.started":"2023-11-28T15:35:39.682121Z","shell.execute_reply":"2023-11-28T15:35:40.285418Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"\u001b[1mWe have to \u001b[0m Falcon Raptor SN-1.0 on SN-1. We are essentially a steward of life &amp; duty-bound imo to ensure its continuance.  http://t.co/R1d6Zw9hH\n\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"print(prompt_ai.generate(prompt = \"Donald Trump can \"))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:44:04.319874Z","iopub.execute_input":"2023-11-28T15:44:04.320673Z","iopub.status.idle":"2023-11-28T15:44:04.614155Z","shell.execute_reply.started":"2023-11-28T15:44:04.320636Z","shell.execute_reply":"2023-11-28T15:44:04.613080Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"\u001b[1mDonald Trump can \u001b[0m anyplace in the world &amp; I will do some great terms with the Air Force. Major pucker factor for Space Station.\n\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"print(prompt_ai.generate(prompt = \"Tesla will \"))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:43:39.201215Z","iopub.execute_input":"2023-11-28T15:43:39.201579Z","iopub.status.idle":"2023-11-28T15:43:39.437931Z","shell.execute_reply.started":"2023-11-28T15:43:39.201551Z","shell.execute_reply":"2023-11-28T15:43:39.436712Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"\u001b[1mTesla will \u001b[0m charge a cool price for the 75 kWh pack pack &amp; have a fixed price for the pack\n\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"ai.save_for_upload('sd-prompt-generator-gpt-neo')","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:44:21.702487Z","iopub.execute_input":"2023-11-28T15:44:21.703312Z","iopub.status.idle":"2023-11-28T15:44:22.502931Z","shell.execute_reply.started":"2023-11-28T15:44:21.703276Z","shell.execute_reply":"2023-11-28T15:44:22.501775Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"tokenizer config file saved in sd-prompt-generator-gpt-neo/tokenizer_config.json\nSpecial tokens file saved in sd-prompt-generator-gpt-neo/special_tokens_map.json\n","output_type":"stream"}]}]}